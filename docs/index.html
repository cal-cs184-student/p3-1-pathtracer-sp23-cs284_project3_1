<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">I-Lun Tsai, Hsuan-Hao Wang</h2>

<!-- Add Website URL -->
<h2 align="middle"><a href="https://cal-cs184-student.github.io/p3-1-pathtracer-sp23-cs284_project3_1/">Website URL</a></h2>

<br><br>


<!-- <div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div>

<p>All of the text in your write-up should be <em>in your own words</em>. If you need to add additional HTML features to this document, you can search the <a href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions. To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names appropriately.</p>
<o>The website writeup is intended to be a self-contained walkthrough of the assignment: we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images as well as written explanations about what you did to complete each part of the assignment. Try to be as clear and organized as possible when writing about your own output files or extensions to the assignment. We want to understand what you've achieved and how you've done it!</p> 
<p>If you are well-versed in web development, feel free to ditch this template and make a better looking page.</p>


<p>Here are a few problems students have encountered in the past. Test your website on the instructional machines early!</p>
<ul>
<li>Your main report page should be called index.html.</li>
<li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
<li>Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre></li>
<li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre></li>
<li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
<li>And again, test your website on the instructional machines early!</li>
</ul>


<p>Here is an example of how to include a simple formula:</p>
<p align="middle"><pre align="middle">a^2 + b^2 = c^2</pre></p>
<p>or, alternatively, you can include an SVG image of a LaTex formula.</p>

<div> -->

<h2 align="middle">Overview</h2>
<p>
  Path tracing is a method that simulates the behavior of light rays. In this project, we implemented a renderer that uses a path tracing algorithm to render scenes with physical lighting. Specifically, I generated random rays from every pixel of the camera, determined the nearest intersection point for the ray with a primitive in the scene. 
</p>
<p>
  After determining the intersection points, we rendered lighting calculating the amount of light that reached each pixel of the camera which included direct lighting and global illumination. This step was crucial in creating a more realistic illumination in 3D scenes.
</p>
<p> 
  In summary, we were able to simulate the behavior of light rays in a scene, generate realistic illumination in 3D scenes, and speed up the rendering process using optimization techniques such as bounding volume hierarchy.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<!-- <h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3> -->
<h3>
  Ray Generation
</h3>
<p>
  Simulating light involves casting a ray from its origin coordinate, which is the camera's position in the world, toward its direction vector. The process of casting a ray from a pixel contains three coordinate transformations to determine the appropriate ray direction.
</p>
<ol>
  <li>Pixel coordinates frame \(\rightarrow\) normalized coordinate frame (scaling axes)</li>
  <li>Normalized coordinate frame \(\rightarrow\) image frame (translation and scaling)</li>
  <li>Image frame \(\rightarrow\) world frame (camera-to-world transform matrix)</li>
</ol>
<p>
  We can sample a single pixel multiple times by altering the ray direction. After determining the ray direction for the pixel, the direction vector undergoes a camera-to-world transformation, ensuring that the ray propagates correctly in the scene.
</p>
<br>

<!-- <h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3> -->
<h3>
  Primitive Intersection
</h3>
<p>
  We used the Moller-Trumbore Algorithm to detect ray intersections with a triangle primitive without the plane equation on which the triangle is laying. This algorithm utilizes the three vertices of the triangle to find a vector normal to the plane which is further used to calculate the  ray-plane intersection by solving linear equations. Barycentric coordinates are then used to determine whether the intersection point resides inside a triangle primitive. We had to check whether the t-value of the intersection was within the maximum and minimum bounds of the scene. 
</p>
<p>
  For ray-sphere intersections, the intersection t-values were computed by solving a quadratic equation using the quadratic root formula. We checked if either of the roots of the quadratic fell within the maximum and minimum bounds and selected the minimum one.
</p>
<br>

<!-- <h3>
  Show images with normal shading for a few small .dae files.
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F584fab66-4f56-4769-9930-17170a87ba67%2Ftask1_4.png?id=11b36000-cad5-49cc-992e-de7fcb0685ca&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=1600&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1ba56817-2845-4614-b1b7-02aeee665d6d%2Ftask1_bunny.png?id=a0201efe-1bbd-4d70-8873-34d163144510&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=1600&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1c52cf7c-b8dd-4a48-890c-45d092d30787%2Ftask1_coil.png?id=2c0ca2a6-de1b-431e-8a1a-d8b99d1e5488&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=1600&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0739cc9a-4cc1-4e7c-bcb7-f97edcadf477%2Ftask1_gems.png?id=f5ffde6d-6bce-4037-98fd-1a108fa9739a&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=1600&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<!-- <h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
    YOUR RESPONSE GOES HERE
</p> -->
<p>
  Our algorithm in constructing a Bounding Volume Hierarchy (BVH) is as follows:
</p>
<ol>
  <li>Computed the mean 3D position of the centroids of each primitive within the node’s bounding box. The \((x, y, z)\) coordinates are the 3 candidate planes along which to split.</li>
  <li>Find which split would result in the minimum (surface area for node’s bounding box \(\times\) number of primitives) for both left and right child nodes.</li>
  <li>Recursively construct the BVH for the left and right nodes until leaf node contains at most max_leaf_size primitives.</li>
</ol>
<p>
  We reason that selecting the minimum surface area from the 3 axes rather than a random axis along which to split would result in bounding box filled densely with primitives. Therefore, the probability that a ray would hit the bounding box of a BVH and also hit a primitive was largely increased.
</p>
<p>
  The computational efficiency is significantly increased by BVH. Theoretically, the average number of intersection tests per ray is O(n) without BVH and O(logn) with a BVH, where n is the number of primitives in the scene. In order to render the image of a cow below, approximately 872 ray intersection tests were performed without the BVH compared to only 4 intersection tests with the BVH. In terms of rendering time, the process took 41.9231 seconds to complete without the BVH compared to 0.0362 seconds with the BVH.  
</p>

<!-- <h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F097e1b4b-cb3b-4a0d-98d5-1f85acb8772b%2FUntitled.png?id=aeb7b92c-3f89-4734-8005-5949aced067a&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=2000&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fcb3923e6-c0f2-41cb-a3ca-fd86720afd22%2Fmaxplanck_screenshot_3-13_21-8-43.png?id=035c8ee9-1c10-4f81-9d60-cab8c099c9c6&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=2000&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F67e1c5e6-3a5a-4f05-ad8c-90bb6a6e6417%2Fcow_screenshot_3-13_21-7-25.png?id=b9e8bcb9-e072-40b7-a4d5-a0f45b0b5d96&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=2000&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
      <td>
        <img src="https://foil-valley-a41.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F19da071b-ab94-4dc6-8ecc-62ac59d0f103%2FCBlucy_screenshot_3-13_21-9-51.png?id=92404fc8-bda2-4b44-975d-2bfe33b429d6&table=block&spaceId=ff67df48-404c-4889-a983-fd4c22d51181&width=2000&userId=&cache=v2" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<!-- <h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
    YOUR RESPONSE GOES HERE
</p>
<br> -->

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<!-- <h3>
  Walk through both implementations of the direct lighting function.
</h3> -->
<p>
  Two ways to achieve direct lighting are uniform hemisphere sampling and light importance sampling. Uniform hemisphere sampling utilizes a probability distribution function (pdf) that evenly distributes the probability among all points in the hemisphere. To simulate a bounce from a light source to a surface and back to the camera, we randomly generate a vector from this hemisphere and then check if the resulting ray intersects any light sources. On the other hand, light importance sampling involves sampling over the light sources and checking if there is an unobstructed path for a ray to travel between the light and the surface. By utilizing the rays and light values, we can calculate the radiance produced on the surface by each light source and then average over all the light sources.
</p>

<!-- <h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="./images/p3/CBbunny_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="./images/p3/CBbunny_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="./images/p3/CBdragon_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
      <td>
        <img src="./images/p3/CBdragon_64_32.png" align="middle" width="400px"/>
        <figcaption>CBdragon.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<!-- <h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="./images/p3/dragon_1_1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray</figcaption>
      </td>
      <td>
        <img src="./images/p3/dragon_1_4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="./images/p3/dragon_1_16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays</figcaption>
      </td>
      <td>
        <img src="./images/p3/dragon_1_64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  Since we take multiple samples for non-point light sources, the randomness arises from the location differences along the light source. As the number of samples increases, the sensitivity of the surface reflectance to the variance in the distance from the sampled light source decreases. With lower sampling rates, the chances of accidentally sampling vectors that are not representative of the lighting of the point increase. For example, we might end up sampling a shaded point when most of the light reaches the point. However, as we increase the number of samples, we account for all of these vectors more effectively, which results in a more noticeable shade gradient.
</p>
<br>

<!-- <h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3> -->
<p>
  Uniform hemisphere sampling yields a higher noise level, giving equal weight to samples from all locations across the hemisphere. Uniform hemisphere sampling is impractical for environments with point lights because the sampled rays will not point directly to the light. Compared to light importance sampling, the environment tends to have more noisy light points. Light importance sampling produces a more consistent image with an observable level of consistency. For example, in a single-bounce light-sampled image, the ceiling is always completely dark, as the rays between the surface and the light are always regarded as being shadowed. In general, light importance sampling results in a smoother and more consistent image with the geometries, but it has a somewhat artificial look that can be overproduced.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<!-- <h3>
  Walk through your implementation of the indirect lighting function.
</h3> -->
<p>
  Although direct lighting produces decent images, it is unrealistic because it only considers one light bounce. In real life, light bounces multiple times off surfaces, so areas not directly hit by light can still be illuminated. Therefore, global illumination is needed to account for this.
</p>
<br>

<!-- <h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="./images/p4/global_1.png" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
      <td>
        <img src="./images/p4/global_2.png" align="middle" width="400px"/>
        <figcaption></figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  In this section, the first step in implementing global illumination was to create a method that returns the radiance at zero light bounces. This method determines the light emitted from a light source that travels directly toward the camera. This method uses direct lighting methods created in Part 3, with hemisphere sampling or importance sampling.
</p>
<br>
<p>
  The recursive method for at least one light bounce was then implemented. This method uses Russian Roulette to terminate sampling randomly to decrease the rendering computations required for the scene. The method starts by setting a vector L_out, which is used to accumulate radiances. The maximum ray depth is then checked to ensure it is greater than one. If the pdf is zero, dividing by it can lead to Unsavory White Specks. Also, if the new ray shot from the current point does not intersect with the BVH, continuing is useless. If these conditions are met, the method checks if the current ray depth is the same as the maximum. If it is, another bounce must be guaranteed, so the method accumulates L_out with a recursive call to itself. The resulting radiance is scaled by the BSDF and a cosine factor, then divided by the pdf. In this recursive call, the new ray created earlier is used, with a depth value decremented from the current ray's value. This approach eventually terminates the recursion without Russian Roulette because the ray depth will no longer exceed one.
</p>
<br>
<p>
  If the current ray is less than the maximum ray depth, Russian Roulette can be used to terminate the ray randomly. A coin_flip method is called with a termination probability of 0.3 (the same as the continuation probability of 0.7). If Russian Roulette dictates that sampling should continue, the method checks if the current ray's depth is greater than one. If it is, L_out is accumulated with a recursive call to itself, using the new ray created earlier. As before, the resulting radiance is scaled by the BSDF and a cosine factor, then divided by the pdf. However, it is also divided by (1 - termination probability) because of the use of Russian Roulette.
</p>
<br>

<!-- <h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="./images/p4/direct.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination</figcaption>
      </td>
      <td>
        <img src="./images/p4/indirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  The images of CBbunny.dae displayed below reveals that increasing the maximum ray depth leads to an increase in the scene's light. Initially, with zero bounces, the only visible light is emitted from the light source. After one bounce, the resulting image is almost identical to that obtained in Part 3, where direct lighting and importance sampling was used, except for the visible emitted light at the top. As the maximum ray depth is increased, the resulting image becomes progressively brighter, and more colored light is seen on uncolored surfaces. It is important to note that, despite the probabilistic random termination used in our implementation, the render time for images at higher maximum ray depths did not increase significantly compared to images at lower maximum ray depths. Even with a maximum ray depth of 100, the likelihood of the at_least_one_bounce method being recursed 99 times is extremely low (in our implementation, the probability of this happening is (0.7)99, which is a very small number).
</p>
<br>

<!-- <h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="./images/p4/depth0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="./images/p4/depth1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="./images/p4/depth2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="./images/p4/depth3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="./images/p4/depth4.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  We can see that increasing sample size decreases the noise of the render from the below comparisons.
</p>
<br>

<!-- <h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3> -->
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="./images/p4/sample1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (sphere.dae)</figcaption>
      </td>
      <td>
        <img src="./images/p4/sample2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (sphere.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="./images/p4/sample4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (sphere.dae)</figcaption>
      </td>
      <td>
        <img src="./images/p4/sample8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (sphere.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="./images/p4/sample16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (sphere.dae)</figcaption>
      </td>
      <td>
        <img src="./images/p4/sample64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (sphere.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="./images/p4/sample1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (sphere.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<!-- <br>
<p>
    YOUR EXPLANATION GOES HERE
</p> -->
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<!-- <h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3> -->
<p>
  Adaptive sampling is a method that intelligently stops processing rays per pixel by analyzing the convergence behavior of the pixel's illuminance. If a pixel appears to have converged to a stable value, then no additional sampling is necessary to render the scene accurately. The implementation of this technique involves adjusting the raytrace_pixel() function so that it ends its sample-averaging loop before reaching the maximum number of samples, provided that the conditions for illuminance convergence have been met. These conditions are based on the average illuminance, the variance in the samples, and z-scores, assuming that the probability distribution of illuminance in the pixel's footprint is Gaussian. This technique improves computational efficiency by optimizing the amount of pixel super-sampling, and it is similar to Mipmap architectures because it combines the information variation within a single pixel's scene footprint with the degree of super-sampling required for that pixel.
</p>
<p>
  The image generated using adaptive sampling had a maximum of 2048 samples per pixel, a ray depth of 5 for global illumination, and a sampling batch size of 64 rays. Upon observing the sample rate map, it was noticed that regions in the scene with more complex surfaces and prominent contrast gradients required more samples per pixel. This is due to the higher variance of samples in these areas, resulting in a longer convergence time. The use of adaptive sampling significantly reduced the render time to 296.2882 seconds, processing 370,872,646 rays, as compared to the non-adaptive approach which took 937.0081 seconds and processed 1,238,167,052 rays. These results clearly demonstrate the computational advantages of adaptive sampling, which reduces the number of processed rays by almost an order of magnitude without compromising the quality of the rendered image. Comparing the final images rendered with and without adaptive sampling, both methods produce almost identical outputs.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Rendered image (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Rendered image (example2.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (example2.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
